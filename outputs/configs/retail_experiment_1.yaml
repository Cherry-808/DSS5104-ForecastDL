experiment_name: retail_s1_i1_comparison_v1

# --- Dataset Configuration ---
dataset:
  name: dataset_retail # Identifier for the dataset type
  raw_dir: data/dataset_retail/raw
  processed_dir: data/dataset_retail/processed
  store_id: 1
  item_id: 1
  validation_cutoff_date: '2017-09-30'
  target_col: 'sales'
  use_log_transform: False # Matches the setting in training scripts

# --- Output Directories ---
output_base_dir: outputs/results/retail # Base directory for results for this dataset
models_dir: models # General directory for saving model files/scalers
# Specific results will go into output_base_dir / experiment_name / model_name / ...

# --- Models to Run ---
models:
  #prophet:
    #params:
      #seasonality_mode: 'additive'
  #arima:
    #params:
     # order: [7, 1, 1] # Example non-seasonal (p,d,q) - TUNE THIS!
      #seasonal_order: [1, 0, 1, 7] # Example seasonal (P,D,Q,m) for weekly - TUNE THIS!
      # use_auto_arima: false # Add flag to control auto_arima usage

  transformer:
    params:
      sequence_length: 60
      head_size: 128
      num_heads: 4
      ff_dim: 128
      num_transformer_blocks: 2
      mlp_units: 64
      dropout: 0.1
      mlp_dropout: 0.1
      learning_rate: 0.001
      epochs: 1 # Training script might override with its default if not specified
      batch_size: 64
      early_stopping_patience: 10
      

  #xgboost:
    #params:
    #  n_estimators: 500
    #  learning_rate: 0.05
    #  max_depth: 7
    ##  subsample: 0.7
     # colsample_bytree: 0.7
    #  early_stopping_rounds: 25
      # seed: 42 (already in DEFAULT_XGB_PARAMS)

  #lstm:
  #  params:
  #    sequence_length: 60
  #    lstm_units: [64, 32] # Two layers
  #    dropout: 0.1
  #    recurrent_dropout: 0.1
  #    learning_rate: 0.001
  #    epochs: 1
  #    batch_size: 64
  #    early_stopping_patience: 10

 # tcn:
 #   params:
 #     sequence_length: 60
  #    nb_filters: 64
  #    kernel_size: 3
  #    dilations: [1, 2, 4, 8, 16] # Example dilation factors
  #    dropout_rate: 0.1
   #   learning_rate: 0.001
  #    epochs: 1
  #    batch_size: 64
  #    early_stopping_patience: 10



# --- Model Hyperparameters (Example for Prophet & Transformer) ---
# These could be nested under each model name for more complex configs

    # Add other Prophet params if needed, training script needs to read these
  

# Add hyperparameters for other models (xgboost_params, etc.)

# --- Evaluation ---
metrics_to_calculate: ['RMSE', 'MSE', 'MAE', 'MAPE', 'R2', 'Adj_R2']
